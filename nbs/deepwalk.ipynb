{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWalk\n",
    "\n",
    "https://arxiv.org/abs/1403.6652\n",
    "\n",
    "Paper and algorithm authored by Bryan Perozzi, Rami Al-Rfou, Steven Skiena\n",
    "\n",
    "Implementation by Rob Forgione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryLogisticRegression(object):\n",
    "#     def __init__(self, dims, lr):\n",
    "#         self.dims = dims\n",
    "#         self.params = np.random.uniform(0., 1., self.dims)\n",
    "#         self.lr = lr\n",
    "        \n",
    "#     def predict(embedding: np.array) -> np.array:\n",
    "#         y = np.dot(self.params, embedding)\n",
    "#         return BinaryLogisticRegression._sigmoid(y)\n",
    "    \n",
    "#     def parameter_update(self, gradient, lr):\n",
    "#         self.params = self.params - lr * gradient\n",
    "    \n",
    "#     def _sigmoid(self, x):\n",
    "#         return 1./(1+np.exp(-x))   \n",
    "    \n",
    "#     def _d_sigmoid(self, x):\n",
    "#         u = np.dot(self.params, x)\n",
    "#         dsigmoid_du = self._sigmoid(u)*(1 - self._sigmoid(u))\n",
    "#         du_dparams = x * dsigmoid_du\n",
    "#         self.parameter_update(du_dparams, lr)\n",
    "#         return self.params * dsigmoid_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(ABC): \n",
    "    @staticmethod\n",
    "    def merge(a, b):\n",
    "        return InternalNode(a.dims, a, b, None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_tree(nodes):\n",
    "        while len(nodes) > 1:\n",
    "            nodes = [Tree.merge(nodes[i], nodes[i+1]) for i in range(0, len(nodes) - 1, 2)]\n",
    "        \n",
    "    def set_parent(t):\n",
    "        self.parent = t\n",
    "        \n",
    "    def set_left(): self.is_right = False\n",
    "        \n",
    "    def set_right(): self.is_right = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalNode(Tree):\n",
    "    def __init__(self, dims, lr, batch_size, left=None, right=None, parent=None, is_right=None):\n",
    "        self.dims = dims\n",
    "        self.set_left_child(left, left=True)\n",
    "        self.set_right_child(right, left=False)\n",
    "        self.set_parent(parent)\n",
    "        self.is_right = is_right\n",
    "        self.params = np.random.uniform(self.dims) \n",
    "        self.gradients = []\n",
    "        self.lr = 0.01\n",
    "        self.batch_size=64\n",
    "        \n",
    "    def set_left_child(child: Tree):\n",
    "        self.left = child\n",
    "        self.left.set_parent(self)\n",
    "        self.left.set_left()\n",
    "            \n",
    "    def set_right_child(child: Tree):\n",
    "        self.right = child\n",
    "        self.right.set_parent(self)\n",
    "        self.right.set_right()\n",
    "            \n",
    "    def set_parent(parent: Tree):\n",
    "        self.parent = parent    \n",
    "    \n",
    "    def update_gradient(gradient: np.array):\n",
    "        self.gradients.append(gradient)\n",
    "        if len(gradients) >= self.batch_size:\n",
    "            self.params = self.lr * np.stack(self.gradients, axis=0).mean(axis=0)\n",
    "        self.gradients = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf(Tree):\n",
    "    def __init__(self, vertex, parent: InternalNode = None, is_right):\n",
    "        self.parent = parent\n",
    "        self.is_right = is_right \n",
    "        self.vertex = vertex\n",
    "        \n",
    "    def update(self, embedding):\n",
    "        node = self\n",
    "        gradient = np.random.uniform(size=len(embedding))\n",
    "        while node.parent is not None:\n",
    "            is_right = node.is_right\n",
    "            node = node.parent        \n",
    "            d = embedding.dot(node.params) if is_right else -embedding.dot(node.params)\n",
    "            sigma = 1/(1+np.exp(-d)) # if we're a right child, gets p(x=1); else, gets p(x=0)\n",
    "            u = (1+np.exp(-d))*sigma*(1-sigma)\n",
    "            gradient += u*node.params\n",
    "            node.update_gradients(u*embedding)\n",
    "        gradient += u*node.params\n",
    "        self.vertex.update_gradients(sum(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of hierarchical softmax is to make the scoring function run in $O(logv)$ rathern than $O(v)$ by organizing the nodes as a binary tree with a binary classifier at each internal node. At a high level, we follow these steps:\n",
    "1. We identify a leaf that is contained within the window of our vertex within the current random walk\n",
    "2. We take that leaf's parent and compute the probability of having followed the correct path (left or right) to the leaf we identified in step 1 by using the model parameters for this internal node combined with the features for the current vertex (which is a row in $\\Phi$).\n",
    "3. We repeat step 2 for all internal nodes until we get to the root\n",
    "4. The product of all of the internal probabilities gives us the probability of seeing a co-occurrence of the neighbor node given what we know about the node we're exploring\n",
    "5. $-logPr(u_k|\\Phi(v_j))$ is our loss function, where $Pr(u_k|\\Phi(v_j))$ is the probability we calculated in step 4\n",
    "6. We use the loss in step 5 to perform a gradient descent step updating both the parameters of our model and $\\Phi(v_j)$:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha_\\theta * \\frac{\\partial J}{\\partial \\theta}\n",
    "\\\\ \n",
    "\\Phi \\leftarrow \\Phi - \\alpha_\\Phi * \\frac{\\partial J}{\\partial \\Phi}\n",
    "$$\n",
    "\n",
    "Where $\\theta$ represents all of the parameters of all of the models in the internal nodes of the tree, and $\\Phi$ represents the latent representation of the current vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(phi, W, window_size):\n",
    "    for i in range(len(W)):\n",
    "        v = W[i]\n",
    "        idx_lower = np.min(i - window_size, 0)\n",
    "        idx_upper = np.max(i + window_size, len(W))\n",
    "        neighbors = W[idx_lower:idx_upper] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepwalk(g, window_size, embedding_size, walks_per_vertex, walk_length):\n",
    "    phi = np.random.uniform(0., 1., (g.size, embedding_size))\n",
    "    for i in range(0, walks_per_vertex): \n",
    "        shuffled_nodes = g.shuffle().nodes()\n",
    "        for v in shuffled_nodes:\n",
    "            W = random_walk(g, v, walk_length)\n",
    "            skipgram(phi, W, window_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
